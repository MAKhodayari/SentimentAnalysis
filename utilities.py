import numpy as np
import pandas as pd
import sys 
import re
from hazm import word_tokenize, Normalizer
normalizer = Normalizer()

def load_data():
    dataset = pd.DataFrame()
    i = 0
    for i in range(2,66):
        if i != 3:
            data = pd.read_excel('./dataset\\split_'+str(i)+'.xlsx')
            data = data[['comment','label']]
            dataset = dataset.append(data)
        i += 1

    dataset=dataset.dropna()
    a = [0,1,2,3,4,5,6,7,8,9,22]
    dataset=dataset[~dataset['comment'].isin(a)]    
    return dataset

def remove_stop_words(comments):
    stop_words = ['?', '??', '??', '??', '??', '???', '??', '??', '???', '????', '??', '??', '???', '??', '???', '??', '??', '???', '???', '??\u200c???', '??', '??', '????', '??', '???', '??', '???', '????', '??', '????', '???', '??', '????', '??', '????', '??\u200c???', '???', '???', '????', '???', '???', '???_???', '??', '???', '???', '???', '???', '???', '?????', '??', '????', '????', '????', '??', '??', '??', '???', '??\u200c????', '????', '?????', '????', '?????', '???', '??????', '???', '???', '??', '????', '???', '???', '????', '???', '????', '?????', '???', '???', '????', '???', '???', '????', '????', '?????', '??\u200c???', '???', '??', '????_???', '????', '???', '???', '????', '???', '?????', '????', '??????', '???', '??????', '??\u200c?????', '????', '???', '?????', '???', '????', '??', '????', '????', '????', '?????', '????', '??\u200c????', '?????', '?????_??', '????', '????', '????', '???', '??\u200c????', '????', '??', '????', '???', '??????', '????', '?????', '????', '????', '????', '????_???', '???', '??\u200c????', '????', '????', '??', '???', '????', '?????', '????', '?????', '???', '????', '???', '???', '?????', '???', '????', '??', '????', '???', '????_???', '????', '???', '????', '????', '???', '?????', '???', '????', '????', '?????', '???', '???', '????', '????', '????', '????', '???', '?????', '?????', '???', '????', '?????', '???', '????', '????', '???', '?????', '???', '??\u200c????', '????', '??', '?????', '??', '???', '??\u200c???', '????', '???', '?????', '???', '????', '???', '?????', '????', '?????', '????', '????', '???', '?????', '????', '??????', '????', '????', '?????', '???_???', '???', '????', '?????_????', '?????_???', '???', '????', '?????', '?????', '????', '????', '??\u200c??????', '?????', '?????', '????', '????????', '??\u200c????', '????', '??????', '????', '???', '????', '????', '????', '??????', '?????', '???_???', '???', '????', '???', '????', '???', '??\u200c???', '????', '?????', '????', '???', '??????', '????', '????', '????', '?????', '?????', '????', '????_???', '??', '????', '?????', '??\u200c???', '?????', '????', '??\u200c????', '?????', '????', '??????', '???', '??\u200c??', '?????', '????', '?????_???', '????', '??\u200c???', '????', '????', '????', '????', '?????', '???', '??????', '?????', '??\u200c????', '????', '???', '?????', '???', '??', '???', '??\u200c???', '?????', '???', '??', '?????_???', '????', '?????', '???', '?????', '???', '???', '??????', '???', '????', '????', '???', '????', '?????', '??????', '???', '????', '?????_???', '???', '???', '????', '????', '????', '???\u200c???', '?????', '??\u200c?????', '????', '????', '????', '??????', '??', '????', '??????', '????_???', '??', '???', '???', '???????', '????', '??????', '?????', '???', '???', '???\u200c???', '?????', '?????', '????', '???', '???', '?????', '????_???', '????', '????', '????', '??', '????', '?????_?????', '?????', '???', '???', '??????', '?????', '?????', '?????', '????', '????', '?????', '????', '?', '?????', '???', '????', '????', '????', '???', '???', '????', '????????', '?????', '???', '?????', '??', '??????', '??\u200c?????', '??????', '????', '??????', '?????_???', '??', '????????', '???', '????', '????', '??????', '??\u200c????', '???', '????', '????', '?????']
    REPLACE_NO_SPACE = re.compile("[.`;:!\'?,\"()\[\]?????????]")
    REPLACE_NUMBER_ENGLISH = re.compile("[0-9A_Za-z?-?]")
    clean_comments = []
    comments = [REPLACE_NO_SPACE.sub("", line) for line in comments]
    comments = [REPLACE_NUMBER_ENGLISH.sub("", line) for line in comments]
    for review in comments:
        clean_comments.append(
            ' '.join([word for word in review.split() 
                      if word not in stop_words])
        )
    return clean_comments

def tokenize_text(text):
    
    text = normalizer.normalize(text)
    text = text.replace('.', ' ')
    text = re.sub('\s+', ' ', text).strip()
    text = text.replace('\u200c', ' ').replace('\n', '').replace('\r', '').replace('í', '?').replace('ß', '˜')
    tokens = word_tokenize(text)
    return tokens

def create_word_set(comments):
    word_set = set()
    for comment in comments:
        for token in comment:
            word_set.add(token)
    return word_set  

dataset=load_data()
X=dataset.iloc[:, :-1].values
y=dataset.iloc[:, -1].values    

comment=[]
for i in range(len(X)):
    comment.append(remove_stop_words((X[i])))

token=[]
for j in comment:
    token.append(tokenize_text(j[0]))

word_set=create_word_set(token)
print(len(list(word_set)))    