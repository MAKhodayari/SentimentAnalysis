{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7df4937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from hazm import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a67db80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "lemmatizer = Lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "843fa480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datatset():\n",
    "    dataset = pd.read_csv('dataset/ProjectData.csv')\n",
    "    dataset = dataset[['comment','label']]\n",
    "    dataset = dataset[dataset['label'] != -2]\n",
    "    dataset = dataset.dropna()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec5f1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>گردن بند خوبو قشنگیه خوبم جلوه میکنه و خودشو ن...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>به نظر من اصلا خوب نبود! به جاش با روغن زیتون ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>من خریدم مبلم رو بی ریخت کرد و زود پاره شد</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>حتما پیشنهاد میکنم</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>در کل عالی</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62933</th>\n",
       "      <td>کواد کوپتر پرواز دادنش خیلی  لذت بخش هست به شر...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62934</th>\n",
       "      <td>سلاممن ازش خیلس راضی هستم شارژ 10000 واقعی خیل...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62935</th>\n",
       "      <td>این فیلتر رو تا بحال دو بار نصب و تعویض کردم. ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62936</th>\n",
       "      <td>ضد آفتاب مناسبی برای پوست چرب با رنگ‌خوب، البت...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62937</th>\n",
       "      <td>امروز سفارش به دستم رسید. ۳ تا از پایه‌های میز...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62820 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  label\n",
       "0      گردن بند خوبو قشنگیه خوبم جلوه میکنه و خودشو ن...    3.0\n",
       "1      به نظر من اصلا خوب نبود! به جاش با روغن زیتون ...    1.0\n",
       "2             من خریدم مبلم رو بی ریخت کرد و زود پاره شد    1.0\n",
       "3                                     حتما پیشنهاد میکنم    3.0\n",
       "4                                             در کل عالی    3.0\n",
       "...                                                  ...    ...\n",
       "62933  کواد کوپتر پرواز دادنش خیلی  لذت بخش هست به شر...    3.0\n",
       "62934  سلاممن ازش خیلس راضی هستم شارژ 10000 واقعی خیل...    3.0\n",
       "62935  این فیلتر رو تا بحال دو بار نصب و تعویض کردم. ...    3.0\n",
       "62936  ضد آفتاب مناسبی برای پوست چرب با رنگ‌خوب، البت...    3.0\n",
       "62937  امروز سفارش به دستم رسید. ۳ تا از پایه‌های میز...    1.0\n",
       "\n",
       "[62820 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_datatset()\n",
    "dataset['label'] += 2\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dbde331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    40149\n",
       "1.0    17044\n",
       "2.0     5627\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = dataset['label'].value_counts()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dcedef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    text = normalizer.normalize(text)\n",
    "    text = text.replace('.', ' ')\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    text = text.replace('\\u200c', ' ').replace('\\n', '').replace('\\r', '').replace('ي', 'ی').replace('ك', 'ک')\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a5e0fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_set(comments):\n",
    "    word_set = set()\n",
    "    for comment in comments:\n",
    "        for token in comment:\n",
    "            word_set.add(token)\n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bad60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(comment):\n",
    "    stop_words = ['و', 'در', 'به', 'از', 'که', 'این', 'را', 'با', 'است', 'برای', 'آن', 'یک', 'خود', 'تا', 'کرد', 'بر', 'هم', 'نیز', 'گفت', 'می\\u200cشود', 'وی', 'شد', 'دارد', 'ما', 'اما', 'یا', 'شده', 'باید', 'هر', 'آنها', 'بود', 'او', 'دیگر', 'دو', 'مورد', 'می\\u200cکند', 'شود', 'کند', 'وجود', 'بین', 'پیش', 'شده_است', 'پس', 'نظر', 'اگر', 'همه', 'یکی', 'حال', 'هستند', 'من', 'کنند', 'نیست', 'باشد', 'چه', 'بی', 'می', 'بخش', 'می\\u200cکنند', 'همین', 'افزود', 'هایی', 'دارند', 'راه', 'همچنین', 'روی', 'داد', 'سه', 'داشت', 'چند', 'سوی', 'تنها', 'هیچ', 'میان', 'اینکه', 'شدن', 'بعد', 'جدید', 'ولی', 'حتی', 'کردن', 'برخی', 'کردند', 'می\\u200cدهد', 'اول', 'نه', 'کرده_است', 'نسبت', 'بیش', 'شما', 'چنین', 'طور', 'افراد', 'تمام', 'درباره', 'بار', 'بسیاری', 'می\\u200cتواند', 'کرده', 'چون', 'ندارد', 'دوم', 'بزرگ', 'طی', 'حدود', 'همان', 'بدون', 'البته', 'آنان', 'می\\u200cگوید', 'دیگری', 'خواهد_شد', 'کنیم', 'قابل', 'یعنی', 'رشد', 'می\\u200cتوان', 'وارد', 'کل', 'ویژه', 'قبل', 'براساس', 'نیاز', 'گذاری', 'هنوز', 'لازم', 'سازی', 'بوده_است', 'چرا', 'می\\u200cشوند', 'وقتی', 'گرفت', 'کم', 'جای', 'حالی', 'تغییر', 'پیدا', 'اکنون', 'تحت', 'باعث', 'مدت', 'فقط', 'تعداد', 'آیا', 'بیان', 'رو', 'شدند', 'عدم', 'کرده_اند', 'بودن', 'نوع', 'بلکه', 'جاری', 'دهد', 'برابر', 'مهم', 'بوده', 'اخیر', 'مربوط', 'امر', 'زیر', 'گیری', 'شاید', 'خصوص', 'آقای', 'اثر', 'کننده', 'بودند', 'فکر', 'کنار', 'اولین', 'سوم', 'سایر', 'کنید', 'ضمن', 'مانند', 'باز', 'می\\u200cگیرد', 'ممکن', 'حل', 'دارای', 'پی', 'مثل', 'می\\u200cرسد', 'اجرا', 'دور', 'منظور', 'کسی', 'موجب', 'طول', 'امکان', 'آنچه', 'تعیین', 'گفته', 'شوند', 'جمع', 'علاوه', 'گونه', 'تاکنون', 'رسید', 'ساله', 'گرفته', 'شده_اند', 'علت', 'چهار', 'داشته_باشد', 'خواهد_بود', 'طرف', 'تهیه', 'تبدیل', 'مناسب', 'زیرا', 'مشخص', 'می\\u200cتوانند', 'نزدیک', 'جریان', 'روند', 'بنابراین', 'می\\u200cدهند', 'یافت', 'نخستین', 'بالا', 'پنج', 'ریزی', 'چیزی', 'نخست', 'بیشتری', 'ترتیب', 'شده_بود', 'خاص', 'شروع', 'فرد', 'کامل', 'غیر', 'می\\u200cرود', 'دهند', 'آخرین', 'دادن', 'جدی', 'بهترین', 'شامل', 'گیرد', 'بخشی', 'باشند', 'تمامی', 'بهتر', 'داده_است', 'حد', 'نبود', 'کسانی', 'می\\u200cکرد', 'داریم', 'علیه', 'می\\u200cباشد', 'دانست', 'ناشی', 'داشتند', 'دهه', 'می\\u200cشد', 'ایشان', 'آنجا', 'گرفته_است', 'دچار', 'می\\u200cآید', 'لحاظ', 'آنکه', 'داده', 'بعضی', 'هستیم', 'اند', 'برداری', 'نباید', 'می\\u200cکنیم', 'نشست', 'سهم', 'همیشه', 'آمد', 'اش', 'وگو', 'می\\u200cکنم', 'حداقل', 'طبق', 'جا', 'خواهد_کرد', 'نوعی', 'چگونه', 'رفت', 'هنگام', 'فوق', 'روش', 'ندارند', 'سعی', 'بندی', 'شمار', 'کلی', 'کافی', 'مواجه', 'همچنان', 'سمت', 'کوچک', 'داشته_است', 'چیز', 'پشت', 'آورد', 'حالا', 'روبه', 'سال\\u200cهای', 'دادند', 'می\\u200cکردند', 'عهده', 'نیمه', 'جایی', 'دیگران', 'سی', 'بروز', 'یکدیگر', 'آمده_است', 'جز', 'کنم', 'سپس', 'کنندگان', 'خودش', 'همواره', 'یافته', 'شان', 'صرف', 'نمی\\u200cشود', 'رسیدن', 'چهارم', 'یابد', 'متر', 'ساز', 'داشته', 'کرده_بود', 'باره', 'نحوه', 'کردم', 'تو', 'شخصی', 'داشته_باشند', 'محسوب', 'پخش', 'کمی', 'متفاوت', 'سراسر', 'کاملا', 'داشتن', 'نظیر', 'آمده', 'گروهی', 'فردی', 'ع', 'همچون', 'خطر', 'خویش', 'کدام', 'دسته', 'سبب', 'عین', 'آوری', 'متاسفانه', 'بیرون', 'دار', 'ابتدا', 'شش', 'افرادی', 'می\\u200cگویند', 'سالهای', 'درون', 'نیستند', 'یافته_است', 'پر', 'خاطرنشان', 'گاه', 'جمعی', 'اغلب', 'دوباره', 'می\\u200cیابد', 'لذا', 'زاده', 'گردد', 'اینجا']\n",
    "    REPLACE_NO_SPACE = re.compile(\"[.`;:!\\'?,\\\"()\\[\\]،؛ًٌٍَُِّ]\")\n",
    "    REPLACE_NUMBER_ENGLISH = re.compile(\"[0-9A_Za-z۰-۹]\")\n",
    "    comment = [REPLACE_NO_SPACE.sub(\"\", line) for line in comment]\n",
    "    comment = [REPLACE_NUMBER_ENGLISH.sub(\"\", line) for line in comment]\n",
    "    comment = ''.join(c for c in comment if not c.isdigit())\n",
    "    comment = ''.join(c for c in comment if c not in punctuation)\n",
    "    comment = normalizer.normalize(comment)\n",
    "    tokens = word_tokenize(comment)\n",
    "    cleared_text = []\n",
    "    for word in tokens:\n",
    "        word = normalizer.normalize(word)\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        if word not in stop_words and len(word) > 1:\n",
    "            cleared_text.append(word)\n",
    "    return cleared_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c15d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4f99bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in range(len(X)):\n",
    "    tokens.append(preprocessing((X[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3524e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DF(tokens):\n",
    "    word_counts = Counter(word for feature in tokens for word in set(feature))\n",
    "    total_comments = len(tokens)\n",
    "    upper_threshold = total_comments * 0.8\n",
    "    lower_threshold = total_comments * 0.001\n",
    "    pruned_tokens_features = []\n",
    "    for feature in tokens:\n",
    "        pruned_feature = [word for word in feature if word_counts[word] < upper_threshold and word_counts[word] >= lower_threshold]\n",
    "        pruned_tokens_features.append(pruned_feature)\n",
    "    return pruned_tokens_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "517078fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = DF(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fb2c80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF(tokens):\n",
    "    tf = []\n",
    "    for doc in tokens:\n",
    "        doc_tf = {}\n",
    "        for word in doc:\n",
    "            doc_tf[word] = doc.count(word) / len(doc)\n",
    "        tf.append(doc_tf)\n",
    "    idf = {}\n",
    "    for doc in tokens:\n",
    "        for word in set(doc):\n",
    "            if word in idf:\n",
    "                idf[word] += 1\n",
    "            else:\n",
    "                idf[word] = 1\n",
    "    num_docs = len(tokens)\n",
    "    for word in idf:\n",
    "        idf[word] = math.log((1 + num_docs) / (1 + idf[word])) + 1\n",
    "    tfidf = []\n",
    "    for doc in tf:\n",
    "        doc_tfidf = {}\n",
    "        for word in doc:\n",
    "            doc_tfidf[word] = doc[word] * idf[word]\n",
    "        tfidf.append(doc_tfidf)\n",
    "    \n",
    "    # Normalize the TF-IDF score for each word in each document\n",
    "    for i in range(len(tfidf)):\n",
    "        tfidf_values = list(tfidf[i].values())\n",
    "        norm = math.sqrt(sum(x**2 for x in tfidf_values))\n",
    "        for word in tfidf[i]:\n",
    "            tfidf[i][word] /= norm  \n",
    "    vocab = sorted(set(word for doc in tokens for word in doc))\n",
    "    matrix = [[doc.get(word, 0) for word in vocab] for doc in tfidf]\n",
    "    matrix = np.array(matrix)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c6a8fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62820, 1760)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = TF_IDF(tokens)\n",
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a327093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = SelectKBest(chi2, k=300).fit_transform(matrix, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c9fb56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Functions\n",
    "def sigmoid(z):\n",
    "    h = 1 / (1 + np.exp(-z))\n",
    "    return h\n",
    "\n",
    "def logistic_prediction(X, theta, threshold=0.5, norm=False):\n",
    "    z = np.dot(X, theta.T)\n",
    "    h = sigmoid(z)\n",
    "    if not norm:\n",
    "        return h\n",
    "    else:\n",
    "        if threshold == 0.5:\n",
    "            yh = np.array([1 if label >= threshold else 0 for label in h]).reshape(-1, 1)\n",
    "        else:\n",
    "            yh = np.array([1 if label >= threshold else 0 for label in h]).reshape(-1, 1)\n",
    "        return yh\n",
    "\n",
    "def logistic_gradient(X, y, alpha, n_iter):\n",
    "    m_sample, n_feature = X.shape\n",
    "    theta = np.random.rand(n_feature).reshape(-1, n_feature)\n",
    "    iter_cost = []\n",
    "    for i in range(n_iter):\n",
    "        pred = logistic_prediction(X, theta)\n",
    "        change = []\n",
    "        for j in range(n_feature):\n",
    "            change.append((np.dot(X[:, j], (pred - y))) / m_sample)\n",
    "            theta[0][j] = theta[0][j] - alpha * change[j]\n",
    "        cost = abs(sum(change))\n",
    "        iter_cost.append(cost)\n",
    "    return theta[0], np.array(iter_cost)\n",
    "\n",
    "def calc_cross_entropy(X, y, theta):\n",
    "    m_sample = X.shape[0]\n",
    "    ones = np.ones(m_sample)\n",
    "    h = logistic_prediction(X, theta)\n",
    "    ce = -(np.dot(y.T, np.log(h)) + np.dot((ones - y).T, np.log(ones - h))) / m_sample\n",
    "    return ce[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b6dda1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 VS 2:\n",
      "Train Cross Entropy: 0.49 | Test Cross Entropy: 0.51\n",
      "Train Accuracy: 75.83 | Test Accuracy: 74.42\n",
      "\n",
      "1 VS 3:\n",
      "Train Cross Entropy: 0.41 | Test Cross Entropy: 0.42\n",
      "Train Accuracy: 81.44 | Test Accuracy: 80.3\n",
      "\n",
      "2 VS 3:\n",
      "Train Cross Entropy: 0.34 | Test Cross Entropy: 0.34\n",
      "Train Accuracy: 87.73 | Test Accuracy: 88.05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Our Logistic Regression\n",
    "\n",
    "# Preparing Data\n",
    "X_1v2 = X_new[np.where(y != 3)]\n",
    "X_1v3 = X_new[np.where(y != 2)]\n",
    "X_2v3 = X_new[np.where(y != 1)]\n",
    "\n",
    "y_1v2 = y[y != 3]\n",
    "y_1v3 = y[y != 2]\n",
    "y_2v3 = y[y != 1]\n",
    "\n",
    "y_1v2 = np.array([0 if y == 1 else 1 for y in y_1v2]).reshape(-1, 1)\n",
    "y_1v3 = np.array([0 if y == 1 else 1 for y in y_1v3]).reshape(-1, 1)\n",
    "y_2v3 = np.array([0 if y == 2 else 1 for y in y_2v3]).reshape(-1, 1)\n",
    "\n",
    "X_train_1v2, X_test_1v2, y_train_1v2, y_test_1v2 = train_test_split(X_1v2, y_1v2, test_size=0.2, random_state=42)\n",
    "X_train_1v3, X_test_1v3, y_train_1v3, y_test_1v3 = train_test_split(X_1v3, y_1v3, test_size=0.2, random_state=42)\n",
    "X_train_2v3, X_test_2v3, y_train_2v3, y_test_2v3 = train_test_split(X_2v3, y_2v3, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Phase Of 1 VS 2\n",
    "theta_1v2, cost_1v2 = logistic_gradient(X_train_1v2, y_train_1v2, 0.1, 10000)\n",
    "\n",
    "# Test Phase Of 1 VS 2\n",
    "yh_train_1v2 = logistic_prediction(X_train_1v2, theta_1v2, norm=True)\n",
    "yh_test_1v2 = logistic_prediction(X_test_1v2, theta_1v2, norm=True)\n",
    "\n",
    "train_ce_1v2 = calc_cross_entropy(X_train_1v2, y_train_1v2, theta_1v2)\n",
    "test_ce_1v2 = calc_cross_entropy(X_test_1v2, y_test_1v2, theta_1v2)\n",
    "\n",
    "train_acc_1v2 = accuracy_score(y_train_1v2, yh_train_1v2)\n",
    "test_acc_1v2 = accuracy_score(y_test_1v2, yh_test_1v2)\n",
    "\n",
    "# Results For 1 VS 2\n",
    "print('1 VS 2:')\n",
    "print(f'Train Cross Entropy: {train_ce_1v2.round(2)} | Test Cross Entropy: {test_ce_1v2.round(2)}')\n",
    "print(f'Train Accuracy: {round(train_acc_1v2 * 100, 2)} | Test Accuracy: {round(test_acc_1v2 * 100, 2)}\\n')\n",
    "\n",
    "# Train Phase Of 1 VS 3\n",
    "theta_1v3, cost_1v3 = logistic_gradient(X_train_1v3, y_train_1v3, 0.1, 10000)\n",
    "\n",
    "# Test Phase Of 1 VS 3\n",
    "yh_train_1v3 = logistic_prediction(X_train_1v3, theta_1v3, norm=True)\n",
    "yh_test_1v3 = logistic_prediction(X_test_1v3, theta_1v3, norm=True)\n",
    "\n",
    "train_ce_1v3 = calc_cross_entropy(X_train_1v3, y_train_1v3, theta_1v3)\n",
    "test_ce_1v3 = calc_cross_entropy(X_test_1v3, y_test_1v3, theta_1v3)\n",
    "\n",
    "train_acc_1v3 = accuracy_score(y_train_1v3, yh_train_1v3)\n",
    "test_acc_1v3 = accuracy_score(y_test_1v3, yh_test_1v3)\n",
    "\n",
    "# Results For 1 VS 3\n",
    "print('1 VS 3:')\n",
    "print(f'Train Cross Entropy: {train_ce_1v3.round(2)} | Test Cross Entropy: {test_ce_1v3.round(2)}')\n",
    "print(f'Train Accuracy: {round(train_acc_1v3 * 100, 2)} | Test Accuracy: {round(test_acc_1v3 * 100, 2)}\\n')\n",
    "\n",
    "# Train Phase Of 2 VS 3\n",
    "theta_2v3, cost_2v3 = logistic_gradient(X_train_2v3, y_train_2v3, 0.1, 10000)\n",
    "\n",
    "# Test Phase Of 2 VS 3\n",
    "yh_train_2v3 = logistic_prediction(X_train_2v3, theta_2v3, norm=True)\n",
    "yh_test_2v3 = logistic_prediction(X_test_2v3, theta_2v3, norm=True)\n",
    "\n",
    "train_ce_2v3 = calc_cross_entropy(X_train_2v3, y_train_2v3, theta_2v3)\n",
    "test_ce_2v3 = calc_cross_entropy(X_test_2v3, y_test_2v3, theta_2v3)\n",
    "\n",
    "train_acc_2v3 = accuracy_score(y_train_2v3, yh_train_2v3)\n",
    "test_acc_2v3 = accuracy_score(y_test_2v3, yh_test_2v3)\n",
    "\n",
    "# Results For 2 VS 3\n",
    "print('2 VS 3:')\n",
    "print(f'Train Cross Entropy: {train_ce_2v3.round(2)} | Test Cross Entropy: {test_ce_2v3.round(2)}')\n",
    "print(f'Train Accuracy: {round(train_acc_2v3 * 100, 2)} | Test Accuracy: {round(test_acc_2v3 * 100, 2)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "70a3d2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1V2 Train Confusion Matrix:\n",
      " [[12649  1029]\n",
      " [ 3355  1103]]\n",
      "\n",
      "1V3 Train Confusion Matrix:\n",
      " [[ 6240  7316]\n",
      " [ 1177 31021]]\n",
      "\n",
      "2V3 Train Confusion Matrix:\n",
      " [[   66  4461]\n",
      " [   33 32060]]\n",
      "\n",
      "1V2 Test Confusion Matrix:\n",
      " [[3097  269]\n",
      " [ 891  278]]\n",
      "\n",
      "1V3 Test Confusion Matrix:\n",
      " [[1531 1957]\n",
      " [ 297 7654]]\n",
      "\n",
      "2V3 Test Confusion Matrix:\n",
      " [[  20 1080]\n",
      " [  14 8042]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Confusion Matrix\n",
    "print(f'1V2 Train Confusion Matrix:\\n {confusion_matrix(y_train_1v2, yh_train_1v2)}\\n')\n",
    "print(f'1V3 Train Confusion Matrix:\\n {confusion_matrix(y_train_1v3, yh_train_1v3)}\\n')\n",
    "print(f'2V3 Train Confusion Matrix:\\n {confusion_matrix(y_train_2v3, yh_train_2v3)}\\n')\n",
    "\n",
    "# Test Confusion Matrix\n",
    "print(f'1V2 Test Confusion Matrix:\\n {confusion_matrix(y_test_1v2, yh_test_1v2)}\\n')\n",
    "print(f'1V3 Test Confusion Matrix:\\n {confusion_matrix(y_test_1v3, yh_test_1v3)}\\n')\n",
    "print(f'2V3 Test Confusion Matrix:\\n {confusion_matrix(y_test_2v3, yh_test_2v3)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92a73ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Functions\n",
    "def one_hot_encode(y):\n",
    "    m_sample = len(y)\n",
    "    classes = np.unique(y)\n",
    "    c_class = len(classes)\n",
    "    one_hot = np.zeros((m_sample, c_class), int)\n",
    "    for i in range(m_sample):\n",
    "        for j, c in enumerate(classes):\n",
    "            if y[i] == c:\n",
    "                one_hot[i][j] = 1\n",
    "    return one_hot\n",
    "\n",
    "def softmax_prediction(X, theta, norm=False):\n",
    "    z = np.dot(X, theta)\n",
    "    h = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    yh = h / np.sum(h, axis=1, keepdims=True)\n",
    "    if not norm:\n",
    "        return yh\n",
    "    else:\n",
    "        return np.argmax(yh, axis=1, keepdims=True) + 1\n",
    "\n",
    "def softmax_gradient(X, y, alpha, n_iter):\n",
    "    m_sample, n_feature = X.shape\n",
    "    c_class = len(np.unique(y))\n",
    "    theta = np.random.rand(n_feature, c_class)\n",
    "    one_hot = one_hot_encode(np.array(y))\n",
    "    iter_cost = []\n",
    "    for i in range(n_iter):\n",
    "        pred = softmax_prediction(X, theta)\n",
    "        change = np.dot(X.T, (pred - one_hot))\n",
    "        theta = theta - alpha * np.array(change)\n",
    "        cost = 0\n",
    "        for c in range(c_class):\n",
    "            cost += np.sum(one_hot[:, c] * np.log10(pred[:, c]))\n",
    "        cost = -cost / m_sample\n",
    "        iter_cost.append(cost)\n",
    "    return theta, iter_cost[-1].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a78694aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Regression Result:\n",
      "Cross Entropy: 0.23\n",
      "Train Accuracy: 78.99 | Test Accuracy: 78.41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Our Softmax\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
    "\n",
    "class_theta, cross_entropy = softmax_gradient(X_train, y_train, 0.01, 100)\n",
    "\n",
    "yh_train = softmax_prediction(X_train, class_theta, True)\n",
    "yh_test = softmax_prediction(X_test, class_theta, True)\n",
    "\n",
    "acc_train = accuracy_score(np.array(y_train).reshape(-1, 1), yh_train)\n",
    "acc_test = accuracy_score(np.array(y_test).reshape(-1, 1), yh_test)\n",
    "\n",
    "print('Softmax Regression Result:')\n",
    "print(f'Cross Entropy: {cross_entropy}')\n",
    "print(f'Train Accuracy: {round(acc_train * 100, 2)} | Test Accuracy: {round(acc_test * 100, 2)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eef65896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Confusion Matrix:\n",
      " [[10715   226  2754]\n",
      " [ 1648   392  2442]\n",
      " [ 3208   280 28591]]\n",
      "\n",
      "Test Confusion Matrix:\n",
      " [[2637   52  660]\n",
      " [ 423   83  639]\n",
      " [ 848   90 7132]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Confusion Matrix:\\n {confusion_matrix(y_train, yh_train)}\\n')\n",
    "print(f'Test Confusion Matrix:\\n {confusion_matrix(y_test, yh_test)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "05a222c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy Using Library: 79.83\n",
      "Test Accuracy Using Library: 79.46\n",
      "\n",
      "Train Confusion Matrix:\n",
      "[[ 9959    92  3644]\n",
      " [ 1288   219  2975]\n",
      " [ 2017   121 29941]]\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[2437   21  891]\n",
      " [ 337   45  763]\n",
      " [ 532   37 7501]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scikit-Learn Logistic Regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=250)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f'Train Accuracy Using Library: {round(train_accuracy * 100, 2)}')\n",
    "print(f'Test Accuracy Using Library: {round(test_accuracy * 100, 2)}')\n",
    "print()\n",
    "print(f'Train Confusion Matrix:\\n{confusion_matrix(y_train, y_train_pred)}\\n')\n",
    "print(f'Test Confusion Matrix:\\n{confusion_matrix(y_test, y_test_pred)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4f3d4efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Functions\n",
    "def class_probability(label):\n",
    "    m_sample = len(label)\n",
    "    _, count = np.unique(label, return_counts=True)\n",
    "    class_prob = count / m_sample\n",
    "    return class_prob\n",
    "\n",
    "def index_probability(data, label):\n",
    "    unique, count = np.unique(label, return_counts=True)\n",
    "    c_class = len(unique)\n",
    "    m_sample = len(label)\n",
    "    n_feature = 28 * 28\n",
    "    index_prob = np.zeros((n_feature, c_class, 2))\n",
    "    for m in range(m_sample):\n",
    "        for i in range(300):\n",
    "            if data[m][i] == 0:\n",
    "                index_prob[i][label[m]][0] += 1\n",
    "            elif data[m][i] != 0:\n",
    "                index_prob[i][label[m]][1] += 1\n",
    "    for n in range(n_feature):\n",
    "        for c in range(c_class):\n",
    "            for v in range(2):\n",
    "                index_prob[n][c][v] = index_prob[n][c][v] / count[c]\n",
    "    return index_prob\n",
    "\n",
    "def naive_bayes_prediction(data, c_prob, i_prob):\n",
    "    c_class = len(c_prob)\n",
    "    m_sample = len(data)\n",
    "    data_pred = np.zeros((m_sample, 1), int)\n",
    "    for i, d in enumerate(data):\n",
    "        pred_arr = c_prob.copy()\n",
    "        for j in range(300):\n",
    "            for m in range(c_class):\n",
    "                if d[j] == 0:\n",
    "                    pred_arr[m] *= i_prob[j][m][0]\n",
    "                elif d[j] != 0:\n",
    "                    pred_arr[m] *= i_prob[j][m][1]\n",
    "        data_pred[i][0] = np.argmax(pred_arr)\n",
    "    return data_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3da6f7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 77.79\n",
      "Test Accuracy: 77.46\n"
     ]
    }
   ],
   "source": [
    "# Our Naive Bayes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
    "y_train = np.array(y_train.astype(int) - 1)\n",
    "y_test = np.array(y_test.astype(int) - 1)\n",
    "\n",
    "class_prob = class_probability(y_train)\n",
    "index_prob = index_probability(X_train.tolist(), y_train)\n",
    "\n",
    "data_pred_train = naive_bayes_prediction(X_train, class_prob, index_prob)\n",
    "data_pred_test = naive_bayes_prediction(X_test, class_prob, index_prob)\n",
    "\n",
    "naive_bayes_acc_train = accuracy_score(y_train.reshape(-1, 1), data_pred_train.reshape(-1, 1))\n",
    "naive_bayes_acc_test = accuracy_score(y_test.reshape(-1, 1), data_pred_test.reshape(-1, 1))\n",
    "\n",
    "print('Train Accuracy: ' + str(round(naive_bayes_acc_train * 100, 2)))\n",
    "print('Test Accuracy: ' + str(round(naive_bayes_acc_test * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c8b38c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Confusion Matrix:\n",
      " [[ 9410   437  3848]\n",
      " [ 1214   613  2655]\n",
      " [ 2004  1006 29069]]\n",
      "\n",
      "Test Confusion Matrix:\n",
      " [[2303  101  945]\n",
      " [ 298  156  691]\n",
      " [ 513  284 7273]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Confusion Matrix:\\n {confusion_matrix(y_train, data_pred_train)}\\n')\n",
    "print(f'Test Confusion Matrix:\\n {confusion_matrix(y_test, data_pred_test)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "559b765f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 75.4\n",
      "Test Accuracy 75.45\n",
      "\n",
      "Train Confusion Matrix:\n",
      " [[ 6477     2  7216]\n",
      " [  551    13  3918]\n",
      " [  668     7 31404]]\n",
      "\n",
      "Test Confusion Matrix:\n",
      " [[1574    0 1775]\n",
      " [ 135    2 1008]\n",
      " [ 163    3 7904]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scikit-Learn Naive Bayes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "train_accuracy = clf.score(X_train, y_train)\n",
    "test_accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "print(f'Train Accuracy {round(train_accuracy * 100, 2)}')\n",
    "print(f'Test Accuracy {round(test_accuracy * 100, 2)}')\n",
    "print()\n",
    "print(f'Train Confusion Matrix:\\n {confusion_matrix(y_train, clf.predict(X_train))}\\n')\n",
    "print(f'Test Confusion Matrix:\\n {confusion_matrix(y_test, clf.predict(X_test))}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5eff03d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Functions\n",
    "def svm (X,Y,C):\n",
    "    learning_rate = 0.01\n",
    "    _samples,_features = X.shape\n",
    "    w = np.zeros((1, _features))\n",
    "    b = 0\n",
    "    for i in range(600):\n",
    "        gradw = 0\n",
    "        gradb = 0\n",
    "        for idx, x_i in enumerate(X):\n",
    "            if Y[idx] * (np.dot(w, x_i.T) + b)> 1:\n",
    "                gradw += 0\n",
    "                gradb += 0\n",
    "            else:\n",
    "                gradw += C * Y[idx] * x_i\n",
    "                gradb += C * Y[idx]  \n",
    "        w = w - learning_rate * w + learning_rate * gradw\n",
    "        b = b + learning_rate * gradb\n",
    "    return w,b\n",
    "\n",
    "def predict(X,w,b):\n",
    "    prediction = np.dot(X, w[0]) + b \n",
    "    return np.sign(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef88c154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  64.68\n"
     ]
    }
   ],
   "source": [
    "# Our SVM\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Preparing Data\n",
    "X_1v2 = X_train[np.where(y_train != 3)]\n",
    "X_1v3 = X_train[np.where(y_train != 2)]\n",
    "X_2v3 = X_train[np.where(y_train != 1)]\n",
    "\n",
    "y_1v2 = y_train[y_train != 3]\n",
    "y_1v3 = y_train[y_train != 2]\n",
    "y_2v3 = y_train[y_train != 1]\n",
    "\n",
    "y_1v2 = np.array([-1 if y == 1 else 1 for y in y_1v2]).reshape(-1, 1)\n",
    "y_1v3 = np.array([-1 if y == 1 else 1 for y in y_1v3]).reshape(-1, 1)\n",
    "y_2v3 = np.array([-1 if y == 2 else 1 for y in y_2v3]).reshape(-1, 1)\n",
    "\n",
    "w1V2, b1v2 = svm(X_1v2, y_1v2, 100)\n",
    "w1V3, b1v3 = svm(X_1v3, y_1v3, 100)\n",
    "w2V3, b2v3 = svm(X_2v3, y_2v3, 100)\n",
    "\n",
    "y_pre1v2 = predict(X_test, w1V2, b1v2)\n",
    "y_pre1v3 = predict(X_test, w1V3, b1v3)\n",
    "y_pre2v3 = predict(X_test, w2V3, b2v3)\n",
    "\n",
    "y_pre1v2 = [1 if y == -1 else 2 for y in y_pre1v2]\n",
    "y_pre1v3 = [1 if y == -1 else 3 for y in y_pre1v3]\n",
    "y_pre2v3 = [2 if y == -1 else 3 for y in y_pre2v3]\n",
    "\n",
    "# Maximum Majority \n",
    "y_pre = []\n",
    "for y in range(len(y_pre1v2)):\n",
    "    temp = []\n",
    "    temp.append(y_pre1v2[y])\n",
    "    temp.append(y_pre1v3[y])\n",
    "    temp.append(y_pre2v3[y])\n",
    "    label_1 = temp.count(1)\n",
    "    label_2 = temp.count(2)\n",
    "    label_3 = temp.count(3)\n",
    "    if label_1 == max(label_1, label_2, label_3):\n",
    "        y_pre.append(1)\n",
    "        continue\n",
    "    if label_2 == max(label_1, label_2, label_3):\n",
    "        y_pre.append(2) \n",
    "        continue\n",
    "    if label_3 == max(label_1, label_2, label_3):\n",
    "        y_pre.append(3) \n",
    "        continue\n",
    "        \n",
    "accuracy = accuracy_score(y_test, y_pre)  \n",
    "\n",
    "print(\"Accuracy: \", round(accuracy * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a611545f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  29  154 3166]\n",
      " [   1   91 1053]\n",
      " [   0   63 8007]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c647a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  79.15\n",
      "[[  29  154 3166]\n",
      " [   1   91 1053]\n",
      " [   0   63 8007]]\n"
     ]
    }
   ],
   "source": [
    "# Scikit-Learn SVM\n",
    "clf = svm.SVC(kernel='linear', decision_function_shape='ovr')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", round(accuracy * 100, 2))\n",
    "print(confusion_matrix(y_test, y_pre))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
