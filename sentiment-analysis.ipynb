{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7df4937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from hazm import *\n",
    "from stopwords_guilannlp import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from string import punctuation\n",
    "import re\n",
    "import math\n",
    "import xlrd\n",
    "import nltk\n",
    "import math\n",
    "normalizer = Normalizer()\n",
    "lemmatizer = Lemmatizer()\n",
    "stemmer = Stemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "843fa480",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv(\"ProjectData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6116a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset[['comment','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4300241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset[dataset['label'] != -2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20ca073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c201caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersampl_data(dataset):\n",
    "    counts = dataset['label'].value_counts()\n",
    "    min_count = counts.min()\n",
    "    undersampled_df = pd.concat([\n",
    "        dataset[dataset['label'] == Label].sample(n=min_count)\n",
    "        for Label in counts.index\n",
    "    ])\n",
    "    \n",
    "    return undersampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d36a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=undersampl_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dbde331",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = dataset ['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5126f94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1.0    5627\n",
       "-1.0    5627\n",
       " 0.0    5627\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89aaf224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10425</th>\n",
       "      <td>من وقتی بدستم رسید وشارژ اولیه رو زدم تا 35 رو...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26499</th>\n",
       "      <td>خیلی بامزه هست...دقیقا همونی هست که تو تصویره....</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>دستبند خیلی خیلی قشنگیه مخصوصا نگین های مختلفش...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7382</th>\n",
       "      <td>کیفیتش عالی هست و اصلا فکر نمیکنید که برچسب زد...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10954</th>\n",
       "      <td>واقعا گاز خوبیه راضی ام‌ازش</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3964</th>\n",
       "      <td>به قیمتش اصلا ارزنده نیست</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37192</th>\n",
       "      <td>من تقریبا ۱ سال شده که این موس رو دارم و هیچ م...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5081</th>\n",
       "      <td>کفپوش قشنگیه ولی میتونست برای قسمت جلوی ماشین ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46915</th>\n",
       "      <td>سلام دوستانعزیزانی که از لق شدن روپدالی شکایت ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8209</th>\n",
       "      <td>اندازه واقعیش 6000تاهست</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16881 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  label\n",
       "10425  من وقتی بدستم رسید وشارژ اولیه رو زدم تا 35 رو...    1.0\n",
       "26499  خیلی بامزه هست...دقیقا همونی هست که تو تصویره....    1.0\n",
       "1272   دستبند خیلی خیلی قشنگیه مخصوصا نگین های مختلفش...    1.0\n",
       "7382   کیفیتش عالی هست و اصلا فکر نمیکنید که برچسب زد...    1.0\n",
       "10954                        واقعا گاز خوبیه راضی ام‌ازش    1.0\n",
       "...                                                  ...    ...\n",
       "3964                           به قیمتش اصلا ارزنده نیست    0.0\n",
       "37192  من تقریبا ۱ سال شده که این موس رو دارم و هیچ م...    0.0\n",
       "5081   کفپوش قشنگیه ولی میتونست برای قسمت جلوی ماشین ...    0.0\n",
       "46915  سلام دوستانعزیزانی که از لق شدن روپدالی شکایت ...    0.0\n",
       "8209                             اندازه واقعیش 6000تاهست    0.0\n",
       "\n",
       "[16881 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e031d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(comments):\n",
    "    stop_words = ['و', 'در', 'به', 'از', 'که', 'این', 'را', 'با', 'است', 'برای', 'آن', 'یک', 'خود', 'تا', 'کرد', 'بر', 'هم', 'نیز', 'گفت', 'می\\u200cشود', 'وی', 'شد', 'دارد', 'ما', 'اما', 'یا', 'شده', 'باید', 'هر', 'آنها', 'بود', 'او', 'دیگر', 'دو', 'مورد', 'می\\u200cکند', 'شود', 'کند', 'وجود', 'بین', 'پیش', 'شده_است', 'پس', 'نظر', 'اگر', 'همه', 'یکی', 'حال', 'هستند', 'من', 'کنند', 'نیست', 'باشد', 'چه', 'بی', 'می', 'بخش', 'می\\u200cکنند', 'همین', 'افزود', 'هایی', 'دارند', 'راه', 'همچنین', 'روی', 'داد', 'سه', 'داشت', 'چند', 'سوی', 'تنها', 'هیچ', 'میان', 'اینکه', 'شدن', 'بعد', 'جدید', 'ولی', 'حتی', 'کردن', 'برخی', 'کردند', 'می\\u200cدهد', 'اول', 'نه', 'کرده_است', 'نسبت', 'بیش', 'شما', 'چنین', 'طور', 'افراد', 'تمام', 'درباره', 'بار', 'بسیاری', 'می\\u200cتواند', 'کرده', 'چون', 'ندارد', 'دوم', 'بزرگ', 'طی', 'حدود', 'همان', 'بدون', 'البته', 'آنان', 'می\\u200cگوید', 'دیگری', 'خواهد_شد', 'کنیم', 'قابل', 'یعنی', 'رشد', 'می\\u200cتوان', 'وارد', 'کل', 'ویژه', 'قبل', 'براساس', 'نیاز', 'گذاری', 'هنوز', 'لازم', 'سازی', 'بوده_است', 'چرا', 'می\\u200cشوند', 'وقتی', 'گرفت', 'کم', 'جای', 'حالی', 'تغییر', 'پیدا', 'اکنون', 'تحت', 'باعث', 'مدت', 'فقط', 'تعداد', 'آیا', 'بیان', 'رو', 'شدند', 'عدم', 'کرده_اند', 'بودن', 'نوع', 'بلکه', 'جاری', 'دهد', 'برابر', 'مهم', 'بوده', 'اخیر', 'مربوط', 'امر', 'زیر', 'گیری', 'شاید', 'خصوص', 'آقای', 'اثر', 'کننده', 'بودند', 'فکر', 'کنار', 'اولین', 'سوم', 'سایر', 'کنید', 'ضمن', 'مانند', 'باز', 'می\\u200cگیرد', 'ممکن', 'حل', 'دارای', 'پی', 'مثل', 'می\\u200cرسد', 'اجرا', 'دور', 'منظور', 'کسی', 'موجب', 'طول', 'امکان', 'آنچه', 'تعیین', 'گفته', 'شوند', 'جمع', 'علاوه', 'گونه', 'تاکنون', 'رسید', 'ساله', 'گرفته', 'شده_اند', 'علت', 'چهار', 'داشته_باشد', 'خواهد_بود', 'طرف', 'تهیه', 'تبدیل', 'مناسب', 'زیرا', 'مشخص', 'می\\u200cتوانند', 'نزدیک', 'جریان', 'روند', 'بنابراین', 'می\\u200cدهند', 'یافت', 'نخستین', 'بالا', 'پنج', 'ریزی', 'چیزی', 'نخست', 'بیشتری', 'ترتیب', 'شده_بود', 'خاص', 'شروع', 'فرد', 'کامل', 'غیر', 'می\\u200cرود', 'دهند', 'آخرین', 'دادن', 'جدی', 'بهترین', 'شامل', 'گیرد', 'بخشی', 'باشند', 'تمامی', 'بهتر', 'داده_است', 'حد', 'نبود', 'کسانی', 'می\\u200cکرد', 'داریم', 'علیه', 'می\\u200cباشد', 'دانست', 'ناشی', 'داشتند', 'دهه', 'می\\u200cشد', 'ایشان', 'آنجا', 'گرفته_است', 'دچار', 'می\\u200cآید', 'لحاظ', 'آنکه', 'داده', 'بعضی', 'هستیم', 'اند', 'برداری', 'نباید', 'می\\u200cکنیم', 'نشست', 'سهم', 'همیشه', 'آمد', 'اش', 'وگو', 'می\\u200cکنم', 'حداقل', 'طبق', 'جا', 'خواهد_کرد', 'نوعی', 'چگونه', 'رفت', 'هنگام', 'فوق', 'روش', 'ندارند', 'سعی', 'بندی', 'شمار', 'کلی', 'کافی', 'مواجه', 'همچنان', 'سمت', 'کوچک', 'داشته_است', 'چیز', 'پشت', 'آورد', 'حالا', 'روبه', 'سال\\u200cهای', 'دادند', 'می\\u200cکردند', 'عهده', 'نیمه', 'جایی', 'دیگران', 'سی', 'بروز', 'یکدیگر', 'آمده_است', 'جز', 'کنم', 'سپس', 'کنندگان', 'خودش', 'همواره', 'یافته', 'شان', 'صرف', 'نمی\\u200cشود', 'رسیدن', 'چهارم', 'یابد', 'متر', 'ساز', 'داشته', 'کرده_بود', 'باره', 'نحوه', 'کردم', 'تو', 'شخصی', 'داشته_باشند', 'محسوب', 'پخش', 'کمی', 'متفاوت', 'سراسر', 'کاملا', 'داشتن', 'نظیر', 'آمده', 'گروهی', 'فردی', 'ع', 'همچون', 'خطر', 'خویش', 'کدام', 'دسته', 'سبب', 'عین', 'آوری', 'متاسفانه', 'بیرون', 'دار', 'ابتدا', 'شش', 'افرادی', 'می\\u200cگویند', 'سالهای', 'درون', 'نیستند', 'یافته_است', 'پر', 'خاطرنشان', 'گاه', 'جمعی', 'اغلب', 'دوباره', 'می\\u200cیابد', 'لذا', 'زاده', 'گردد', 'اینجا']\n",
    "    REPLACE_NO_SPACE = re.compile(\"[.`;:!\\'?,\\\"()\\[\\]،؛ًٌٍَُِّ]\")\n",
    "    REPLACE_NUMBER_ENGLISH = re.compile(\"[0-9A_Za-z۰-۹]\")\n",
    "    clean_comments = []\n",
    "    comments = [REPLACE_NO_SPACE.sub(\"\", line) for line in comments]\n",
    "    comments = [REPLACE_NUMBER_ENGLISH.sub(\"\", line) for line in comments]\n",
    "    for review in comments:\n",
    "        clean_comments.append(\n",
    "            ' '.join([word for word in review.split() \n",
    "                      if word not in stop_words])\n",
    "        )\n",
    "    return clean_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dcedef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    text = normalizer.normalize(text)\n",
    "    text = text.replace('.', ' ')\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    text = text.replace('\\u200c', ' ').replace('\\n', '').replace('\\r', '').replace('ي', 'ی').replace('ك', 'ک')\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a5e0fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_set(comments):\n",
    "    word_set = set()\n",
    "    for comment in comments:\n",
    "        for token in comment:\n",
    "            word_set.add(token)\n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bad60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(comment):\n",
    "    stop_words = ['و', 'در', 'به', 'از', 'که', 'این', 'را', 'با', 'است', 'برای', 'آن', 'یک', 'خود', 'تا', 'کرد', 'بر', 'هم', 'نیز', 'گفت', 'می\\u200cشود', 'وی', 'شد', 'دارد', 'ما', 'اما', 'یا', 'شده', 'باید', 'هر', 'آنها', 'بود', 'او', 'دیگر', 'دو', 'مورد', 'می\\u200cکند', 'شود', 'کند', 'وجود', 'بین', 'پیش', 'شده_است', 'پس', 'نظر', 'اگر', 'همه', 'یکی', 'حال', 'هستند', 'من', 'کنند', 'نیست', 'باشد', 'چه', 'بی', 'می', 'بخش', 'می\\u200cکنند', 'همین', 'افزود', 'هایی', 'دارند', 'راه', 'همچنین', 'روی', 'داد', 'سه', 'داشت', 'چند', 'سوی', 'تنها', 'هیچ', 'میان', 'اینکه', 'شدن', 'بعد', 'جدید', 'ولی', 'حتی', 'کردن', 'برخی', 'کردند', 'می\\u200cدهد', 'اول', 'نه', 'کرده_است', 'نسبت', 'بیش', 'شما', 'چنین', 'طور', 'افراد', 'تمام', 'درباره', 'بار', 'بسیاری', 'می\\u200cتواند', 'کرده', 'چون', 'ندارد', 'دوم', 'بزرگ', 'طی', 'حدود', 'همان', 'بدون', 'البته', 'آنان', 'می\\u200cگوید', 'دیگری', 'خواهد_شد', 'کنیم', 'قابل', 'یعنی', 'رشد', 'می\\u200cتوان', 'وارد', 'کل', 'ویژه', 'قبل', 'براساس', 'نیاز', 'گذاری', 'هنوز', 'لازم', 'سازی', 'بوده_است', 'چرا', 'می\\u200cشوند', 'وقتی', 'گرفت', 'کم', 'جای', 'حالی', 'تغییر', 'پیدا', 'اکنون', 'تحت', 'باعث', 'مدت', 'فقط', 'تعداد', 'آیا', 'بیان', 'رو', 'شدند', 'عدم', 'کرده_اند', 'بودن', 'نوع', 'بلکه', 'جاری', 'دهد', 'برابر', 'مهم', 'بوده', 'اخیر', 'مربوط', 'امر', 'زیر', 'گیری', 'شاید', 'خصوص', 'آقای', 'اثر', 'کننده', 'بودند', 'فکر', 'کنار', 'اولین', 'سوم', 'سایر', 'کنید', 'ضمن', 'مانند', 'باز', 'می\\u200cگیرد', 'ممکن', 'حل', 'دارای', 'پی', 'مثل', 'می\\u200cرسد', 'اجرا', 'دور', 'منظور', 'کسی', 'موجب', 'طول', 'امکان', 'آنچه', 'تعیین', 'گفته', 'شوند', 'جمع', 'علاوه', 'گونه', 'تاکنون', 'رسید', 'ساله', 'گرفته', 'شده_اند', 'علت', 'چهار', 'داشته_باشد', 'خواهد_بود', 'طرف', 'تهیه', 'تبدیل', 'مناسب', 'زیرا', 'مشخص', 'می\\u200cتوانند', 'نزدیک', 'جریان', 'روند', 'بنابراین', 'می\\u200cدهند', 'یافت', 'نخستین', 'بالا', 'پنج', 'ریزی', 'چیزی', 'نخست', 'بیشتری', 'ترتیب', 'شده_بود', 'خاص', 'شروع', 'فرد', 'کامل', 'غیر', 'می\\u200cرود', 'دهند', 'آخرین', 'دادن', 'جدی', 'بهترین', 'شامل', 'گیرد', 'بخشی', 'باشند', 'تمامی', 'بهتر', 'داده_است', 'حد', 'نبود', 'کسانی', 'می\\u200cکرد', 'داریم', 'علیه', 'می\\u200cباشد', 'دانست', 'ناشی', 'داشتند', 'دهه', 'می\\u200cشد', 'ایشان', 'آنجا', 'گرفته_است', 'دچار', 'می\\u200cآید', 'لحاظ', 'آنکه', 'داده', 'بعضی', 'هستیم', 'اند', 'برداری', 'نباید', 'می\\u200cکنیم', 'نشست', 'سهم', 'همیشه', 'آمد', 'اش', 'وگو', 'می\\u200cکنم', 'حداقل', 'طبق', 'جا', 'خواهد_کرد', 'نوعی', 'چگونه', 'رفت', 'هنگام', 'فوق', 'روش', 'ندارند', 'سعی', 'بندی', 'شمار', 'کلی', 'کافی', 'مواجه', 'همچنان', 'سمت', 'کوچک', 'داشته_است', 'چیز', 'پشت', 'آورد', 'حالا', 'روبه', 'سال\\u200cهای', 'دادند', 'می\\u200cکردند', 'عهده', 'نیمه', 'جایی', 'دیگران', 'سی', 'بروز', 'یکدیگر', 'آمده_است', 'جز', 'کنم', 'سپس', 'کنندگان', 'خودش', 'همواره', 'یافته', 'شان', 'صرف', 'نمی\\u200cشود', 'رسیدن', 'چهارم', 'یابد', 'متر', 'ساز', 'داشته', 'کرده_بود', 'باره', 'نحوه', 'کردم', 'تو', 'شخصی', 'داشته_باشند', 'محسوب', 'پخش', 'کمی', 'متفاوت', 'سراسر', 'کاملا', 'داشتن', 'نظیر', 'آمده', 'گروهی', 'فردی', 'ع', 'همچون', 'خطر', 'خویش', 'کدام', 'دسته', 'سبب', 'عین', 'آوری', 'متاسفانه', 'بیرون', 'دار', 'ابتدا', 'شش', 'افرادی', 'می\\u200cگویند', 'سالهای', 'درون', 'نیستند', 'یافته_است', 'پر', 'خاطرنشان', 'گاه', 'جمعی', 'اغلب', 'دوباره', 'می\\u200cیابد', 'لذا', 'زاده', 'گردد', 'اینجا']\n",
    "    REPLACE_NO_SPACE = re.compile(\"[.`;:!\\'?,\\\"()\\[\\]،؛ًٌٍَُِّ]\")\n",
    "    REPLACE_NUMBER_ENGLISH = re.compile(\"[0-9A_Za-z۰-۹]\")\n",
    "    comment = [REPLACE_NO_SPACE.sub(\"\", line) for line in comment]\n",
    "    comment = [REPLACE_NUMBER_ENGLISH.sub(\"\", line) for line in comment]\n",
    "    comment = ''.join(c for c in comment if not c.isdigit())\n",
    "    comment = ''.join(c for c in comment if c not in punctuation)\n",
    "    comment = normalizer.normalize(comment)\n",
    "    tokens = word_tokenize(comment)\n",
    "    cleared_text = []\n",
    "    for word in tokens:\n",
    "        word = normalizer.normalize(word)\n",
    "#         word = stemmer.stem(word)\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "          #word = re.sub(r'\\d+', '', word)\n",
    "        if word not in stop_words and len(word) > 1:\n",
    "                cleared_text.append(word)\n",
    "    return cleared_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c15d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=(dataset.iloc[:, :-1].values)\n",
    "y=dataset.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4f99bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[]\n",
    "for i in range(len(X)):\n",
    "    tokens.append(preprocessing((X[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "517078fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fb2c80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = []\n",
    "for doc in tokens:\n",
    "    doc_tf = {}\n",
    "    for word in doc:\n",
    "        doc_tf[word] = doc.count(word) / len(doc)\n",
    "    tf.append(doc_tf)\n",
    "\n",
    "# Calculate the inverse document frequency (IDF) for each word\n",
    "idf = {}\n",
    "for doc in tokens:\n",
    "    for word in set(doc):\n",
    "        if word in idf:\n",
    "            idf[word] += 1\n",
    "        else:\n",
    "            idf[word] = 1\n",
    "\n",
    "num_docs = len(tokens)\n",
    "for word in idf:\n",
    "    idf[word] = math.log((1 + num_docs) / (1 + idf[word])) + 1\n",
    "\n",
    "# Multiply the TF and IDF to get the TF-IDF score for each word\n",
    "tfidf = []\n",
    "for doc in tf:\n",
    "    doc_tfidf = {}\n",
    "    for word in doc:\n",
    "        doc_tfidf[word] = doc[word] * idf[word]\n",
    "    tfidf.append(doc_tfidf)\n",
    "\n",
    "# Normalize the TF-IDF score for each word in each document\n",
    "for i in range(len(tfidf)):\n",
    "    tfidf_values = list(tfidf[i].values())\n",
    "    norm = math.sqrt(sum(x**2 for x in tfidf_values))\n",
    "    for word in tfidf[i]:\n",
    "        tfidf[i][word] /= norm\n",
    "\n",
    "# Create a feature matrix\n",
    "vocab = sorted(set(word for doc in tokens for word in doc))\n",
    "matrix = [[doc.get(word, 0) for word in vocab] for doc in tfidf]\n",
    "matrix = np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8894ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(matrix,y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c476a00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6538347645839503\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the logistic regression model\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45ff6dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6428782943440924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d3f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear', decision_function_shape='ovr') # OVR stands for One-vs-Rest\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "04d9ef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b=[]\n",
    "# for j in a:\n",
    "#     b.append(tokenize_text(j[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b5feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=create_word_set(tokens)\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a5b1739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00 -5.47955771e-01]\n",
      " [-3.92523115e-17  2.40249667e-01]\n",
      " [ 1.17756934e-16  8.55661876e-01]\n",
      " [-1.00000000e+00 -5.47955771e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Example Persian text data\n",
    "text_data = [\n",
    "    'این یک متن نمونه است.',\n",
    "    'این یک متن دیگر است.',\n",
    "    'این هم یک متن دیگر است.',\n",
    "    'این یک  یک متن آزمایشی دیگر است.'\n",
    "]\n",
    "\n",
    "# Convert text data to a matrix of word counts\n",
    "vectorizer = CountVectorizer()\n",
    "word_counts = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Apply PCA to reduce the number of features\n",
    "pca = PCA(n_components=2)\n",
    "reduced_features = pca.fit_transform(word_counts.toarray())\n",
    "\n",
    "# Print the reduced features\n",
    "print(reduced_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "96498071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def compute_tf(text):\n",
    "    tf_text = Counter(text)\n",
    "    for i in tf_text:\n",
    "        tf_text[i] = tf_text[i]/float(len(text))\n",
    "    return tf_text\n",
    "\n",
    "def compute_idf(word, corpus):\n",
    "    return math.log10(len(corpus)/sum([1.0 for i in corpus if word in i]))\n",
    "\n",
    "def compute_tfidf(text, corpus):\n",
    "    tfidf_text = {}\n",
    "    tf_text = compute_tf(text)\n",
    "    for word in tf_text:\n",
    "        idf = compute_idf(word, corpus)\n",
    "        tfidf_text[word] = tf_text[word] * idf\n",
    "    return tfidf_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8b81c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from hazm import word_tokenize\n",
    "\n",
    "# Define a function to calculate the TF-IDF scores\n",
    "def calculate_tfidf(texts):\n",
    "    # Tokenize the texts\n",
    "    tokens = [word_tokenize(text) for text in texts]\n",
    "\n",
    "    # Count the number of documents that contain each token\n",
    "    doc_freq = {}\n",
    "    for doc in tokens:\n",
    "        for token in set(doc):\n",
    "            if token not in doc_freq:\n",
    "                doc_freq[token] = 1\n",
    "            else:\n",
    "                doc_freq[token] += 1\n",
    "\n",
    "    # Calculate the IDF scores for each token\n",
    "    idf = {}\n",
    "    num_docs = len(texts)\n",
    "    for token, freq in doc_freq.items():\n",
    "        idf[token] = math.log(num_docs / (freq + 1))\n",
    "\n",
    "    # Calculate the TF-IDF scores for each token in each document\n",
    "    tfidf_scores = []\n",
    "    for doc in tokens:\n",
    "        tf = {}\n",
    "        for token in doc:\n",
    "            if token not in tf:\n",
    "                tf[token] = 1\n",
    "            else:\n",
    "                tf[token] += 1\n",
    "        doc_tfidf = {}\n",
    "        for token, freq in tf.items():\n",
    "            if token in idf:\n",
    "                doc_tfidf[token] = freq * idf[token]\n",
    "        tfidf_scores.append(doc_tfidf)\n",
    "\n",
    "    return tfidf_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58060642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
