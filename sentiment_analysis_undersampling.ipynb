{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df4937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from hazm import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a67db80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "lemmatizer = Lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "843fa480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datatset():\n",
    "    dataset = pd.read_csv('dataset/ProjectData.csv')\n",
    "    dataset = dataset[['comment','label']]\n",
    "    dataset = dataset[dataset['label'] != -2]\n",
    "    dataset = dataset.dropna()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c201caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersampl_data(dataset):\n",
    "    counts = dataset['label'].value_counts()\n",
    "    min_count = counts.min()\n",
    "    undersampled_df = pd.concat([dataset[dataset['label'] == Label].sample(n=min_count)\n",
    "                                 for Label in counts.index])\n",
    "    return undersampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec5f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_datatset()\n",
    "dataset['label'] += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d36a528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57113</th>\n",
       "      <td>بنظرم که کفش خوبیهمن که پوشیدمش واقعا احساس را...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>این کتاب فوق العادست به همه پیشنهاد میکنم</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42098</th>\n",
       "      <td>توپ بسیار خوب و زیبایی است که کنترل آن راحت و ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52830</th>\n",
       "      <td>کیفیتش خیلی عالیهفوق العاده شیک و خوش دستحتما ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39279</th>\n",
       "      <td>خیلی عالیه . واقعا کیفیتشو بعد خرید میبینید . ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53028</th>\n",
       "      <td>بیشتر بدرد مغازه و جای تقریبا ثابت میخوره...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>من اول می خواستم این هدفون رو از دیجی کالا بگی...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12811</th>\n",
       "      <td>معمولیه کلا</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9263</th>\n",
       "      <td>خوبه باحاله فقط رنگش متفاوته باعکس و اینکه قسم...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42245</th>\n",
       "      <td>به نظر من که هدست خوبی هست ولی خیلی ظریفه و نم...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16881 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  label\n",
       "57113  بنظرم که کفش خوبیهمن که پوشیدمش واقعا احساس را...    3.0\n",
       "1503           این کتاب فوق العادست به همه پیشنهاد میکنم    3.0\n",
       "42098  توپ بسیار خوب و زیبایی است که کنترل آن راحت و ...    3.0\n",
       "52830  کیفیتش خیلی عالیهفوق العاده شیک و خوش دستحتما ...    3.0\n",
       "39279  خیلی عالیه . واقعا کیفیتشو بعد خرید میبینید . ...    3.0\n",
       "...                                                  ...    ...\n",
       "53028       بیشتر بدرد مغازه و جای تقریبا ثابت میخوره...    2.0\n",
       "57     من اول می خواستم این هدفون رو از دیجی کالا بگی...    2.0\n",
       "12811                                        معمولیه کلا    2.0\n",
       "9263   خوبه باحاله فقط رنگش متفاوته باعکس و اینکه قسم...    2.0\n",
       "42245  به نظر من که هدست خوبی هست ولی خیلی ظریفه و نم...    2.0\n",
       "\n",
       "[16881 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = undersampl_data(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dbde331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    5627\n",
       "1.0    5627\n",
       "2.0    5627\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = dataset['label'].value_counts()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dcedef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    text = normalizer.normalize(text)\n",
    "    text = text.replace('.', ' ')\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    text = text.replace('\\u200c', ' ').replace('\\n', '').replace('\\r', '').replace('ي', 'ی').replace('ك', 'ک')\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a5e0fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_set(comments):\n",
    "    word_set = set()\n",
    "    for comment in comments:\n",
    "        for token in comment:\n",
    "            word_set.add(token)\n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bad60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(comment):\n",
    "    stop_words = ['و', 'در', 'به', 'از', 'که', 'این', 'را', 'با', 'است', 'برای', 'آن', 'یک', 'خود', 'تا', 'کرد', 'بر', 'هم', 'نیز', 'گفت', 'می\\u200cشود', 'وی', 'شد', 'دارد', 'ما', 'اما', 'یا', 'شده', 'باید', 'هر', 'آنها', 'بود', 'او', 'دیگر', 'دو', 'مورد', 'می\\u200cکند', 'شود', 'کند', 'وجود', 'بین', 'پیش', 'شده_است', 'پس', 'نظر', 'اگر', 'همه', 'یکی', 'حال', 'هستند', 'من', 'کنند', 'نیست', 'باشد', 'چه', 'بی', 'می', 'بخش', 'می\\u200cکنند', 'همین', 'افزود', 'هایی', 'دارند', 'راه', 'همچنین', 'روی', 'داد', 'سه', 'داشت', 'چند', 'سوی', 'تنها', 'هیچ', 'میان', 'اینکه', 'شدن', 'بعد', 'جدید', 'ولی', 'حتی', 'کردن', 'برخی', 'کردند', 'می\\u200cدهد', 'اول', 'نه', 'کرده_است', 'نسبت', 'بیش', 'شما', 'چنین', 'طور', 'افراد', 'تمام', 'درباره', 'بار', 'بسیاری', 'می\\u200cتواند', 'کرده', 'چون', 'ندارد', 'دوم', 'بزرگ', 'طی', 'حدود', 'همان', 'بدون', 'البته', 'آنان', 'می\\u200cگوید', 'دیگری', 'خواهد_شد', 'کنیم', 'قابل', 'یعنی', 'رشد', 'می\\u200cتوان', 'وارد', 'کل', 'ویژه', 'قبل', 'براساس', 'نیاز', 'گذاری', 'هنوز', 'لازم', 'سازی', 'بوده_است', 'چرا', 'می\\u200cشوند', 'وقتی', 'گرفت', 'کم', 'جای', 'حالی', 'تغییر', 'پیدا', 'اکنون', 'تحت', 'باعث', 'مدت', 'فقط', 'تعداد', 'آیا', 'بیان', 'رو', 'شدند', 'عدم', 'کرده_اند', 'بودن', 'نوع', 'بلکه', 'جاری', 'دهد', 'برابر', 'مهم', 'بوده', 'اخیر', 'مربوط', 'امر', 'زیر', 'گیری', 'شاید', 'خصوص', 'آقای', 'اثر', 'کننده', 'بودند', 'فکر', 'کنار', 'اولین', 'سوم', 'سایر', 'کنید', 'ضمن', 'مانند', 'باز', 'می\\u200cگیرد', 'ممکن', 'حل', 'دارای', 'پی', 'مثل', 'می\\u200cرسد', 'اجرا', 'دور', 'منظور', 'کسی', 'موجب', 'طول', 'امکان', 'آنچه', 'تعیین', 'گفته', 'شوند', 'جمع', 'علاوه', 'گونه', 'تاکنون', 'رسید', 'ساله', 'گرفته', 'شده_اند', 'علت', 'چهار', 'داشته_باشد', 'خواهد_بود', 'طرف', 'تهیه', 'تبدیل', 'مناسب', 'زیرا', 'مشخص', 'می\\u200cتوانند', 'نزدیک', 'جریان', 'روند', 'بنابراین', 'می\\u200cدهند', 'یافت', 'نخستین', 'بالا', 'پنج', 'ریزی', 'چیزی', 'نخست', 'بیشتری', 'ترتیب', 'شده_بود', 'خاص', 'شروع', 'فرد', 'کامل', 'غیر', 'می\\u200cرود', 'دهند', 'آخرین', 'دادن', 'جدی', 'بهترین', 'شامل', 'گیرد', 'بخشی', 'باشند', 'تمامی', 'بهتر', 'داده_است', 'حد', 'نبود', 'کسانی', 'می\\u200cکرد', 'داریم', 'علیه', 'می\\u200cباشد', 'دانست', 'ناشی', 'داشتند', 'دهه', 'می\\u200cشد', 'ایشان', 'آنجا', 'گرفته_است', 'دچار', 'می\\u200cآید', 'لحاظ', 'آنکه', 'داده', 'بعضی', 'هستیم', 'اند', 'برداری', 'نباید', 'می\\u200cکنیم', 'نشست', 'سهم', 'همیشه', 'آمد', 'اش', 'وگو', 'می\\u200cکنم', 'حداقل', 'طبق', 'جا', 'خواهد_کرد', 'نوعی', 'چگونه', 'رفت', 'هنگام', 'فوق', 'روش', 'ندارند', 'سعی', 'بندی', 'شمار', 'کلی', 'کافی', 'مواجه', 'همچنان', 'سمت', 'کوچک', 'داشته_است', 'چیز', 'پشت', 'آورد', 'حالا', 'روبه', 'سال\\u200cهای', 'دادند', 'می\\u200cکردند', 'عهده', 'نیمه', 'جایی', 'دیگران', 'سی', 'بروز', 'یکدیگر', 'آمده_است', 'جز', 'کنم', 'سپس', 'کنندگان', 'خودش', 'همواره', 'یافته', 'شان', 'صرف', 'نمی\\u200cشود', 'رسیدن', 'چهارم', 'یابد', 'متر', 'ساز', 'داشته', 'کرده_بود', 'باره', 'نحوه', 'کردم', 'تو', 'شخصی', 'داشته_باشند', 'محسوب', 'پخش', 'کمی', 'متفاوت', 'سراسر', 'کاملا', 'داشتن', 'نظیر', 'آمده', 'گروهی', 'فردی', 'ع', 'همچون', 'خطر', 'خویش', 'کدام', 'دسته', 'سبب', 'عین', 'آوری', 'متاسفانه', 'بیرون', 'دار', 'ابتدا', 'شش', 'افرادی', 'می\\u200cگویند', 'سالهای', 'درون', 'نیستند', 'یافته_است', 'پر', 'خاطرنشان', 'گاه', 'جمعی', 'اغلب', 'دوباره', 'می\\u200cیابد', 'لذا', 'زاده', 'گردد', 'اینجا']\n",
    "    REPLACE_NO_SPACE = re.compile(\"[.`;:!\\'?,\\\"()\\[\\]،؛ًٌٍَُِّ]\")\n",
    "    REPLACE_NUMBER_ENGLISH = re.compile(\"[0-9A_Za-z۰-۹]\")\n",
    "    comment = [REPLACE_NO_SPACE.sub(\"\", line) for line in comment]\n",
    "    comment = [REPLACE_NUMBER_ENGLISH.sub(\"\", line) for line in comment]\n",
    "    comment = ''.join(c for c in comment if not c.isdigit())\n",
    "    comment = ''.join(c for c in comment if c not in punctuation)\n",
    "    comment = normalizer.normalize(comment)\n",
    "    tokens = word_tokenize(comment)\n",
    "    cleared_text = []\n",
    "    for word in tokens:\n",
    "        word = normalizer.normalize(word)\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        if word not in stop_words and len(word) > 1:\n",
    "            cleared_text.append(word)\n",
    "    return cleared_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c15d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4f99bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in range(len(X)):\n",
    "    tokens.append(preprocessing((X[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3524e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DF(tokens):\n",
    "    word_counts = Counter(word for feature in tokens for word in set(feature))\n",
    "    total_comments = len(tokens)\n",
    "    upper_threshold = total_comments * 0.8\n",
    "    lower_threshold = total_comments * 0.001\n",
    "    pruned_tokens_features = []\n",
    "    for feature in tokens:\n",
    "        pruned_feature = [word for word in feature if word_counts[word] < upper_threshold and word_counts[word] >= lower_threshold]\n",
    "        pruned_tokens_features.append(pruned_feature)\n",
    "    return pruned_tokens_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "517078fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = DF(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fb2c80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF(tokens):\n",
    "    tf = []\n",
    "    for doc in tokens:\n",
    "        doc_tf = {}\n",
    "        for word in doc:\n",
    "            doc_tf[word] = doc.count(word) / len(doc)\n",
    "        tf.append(doc_tf)\n",
    "    idf = {}\n",
    "    for doc in tokens:\n",
    "        for word in set(doc):\n",
    "            if word in idf:\n",
    "                idf[word] += 1\n",
    "            else:\n",
    "                idf[word] = 1\n",
    "    num_docs = len(tokens)\n",
    "    for word in idf:\n",
    "        idf[word] = math.log((1 + num_docs) / (1 + idf[word])) + 1\n",
    "    tfidf = []\n",
    "    for doc in tf:\n",
    "        doc_tfidf = {}\n",
    "        for word in doc:\n",
    "            doc_tfidf[word] = doc[word] * idf[word]\n",
    "        tfidf.append(doc_tfidf)\n",
    "    \n",
    "    # Normalize the TF-IDF score for each word in each document\n",
    "    for i in range(len(tfidf)):\n",
    "        tfidf_values = list(tfidf[i].values())\n",
    "        norm = math.sqrt(sum(x**2 for x in tfidf_values))\n",
    "        for word in tfidf[i]:\n",
    "            tfidf[i][word] /= norm  \n",
    "    vocab = sorted(set(word for doc in tokens for word in doc))\n",
    "    matrix = [[doc.get(word, 0) for word in vocab] for doc in tfidf]\n",
    "    matrix = np.array(matrix)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c6a8fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16881, 1920)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = TF_IDF(tokens)\n",
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a327093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = SelectKBest(chi2, k=300).fit_transform(matrix, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c9fb56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Functions\n",
    "def sigmoid(z):\n",
    "    h = 1 / (1 + np.exp(-z))\n",
    "    return h\n",
    "\n",
    "def logistic_prediction(X, theta, threshold=0.5, norm=False):\n",
    "    z = np.dot(X, theta.T)\n",
    "    h = sigmoid(z)\n",
    "    if not norm:\n",
    "        return h\n",
    "    else:\n",
    "        if threshold == 0.5:\n",
    "            yh = np.array([1 if label >= threshold else 0 for label in h]).reshape(-1, 1)\n",
    "        else:\n",
    "            yh = np.array([1 if label >= threshold else 0 for label in h]).reshape(-1, 1)\n",
    "        return yh\n",
    "\n",
    "def logistic_gradient(X, y, alpha, n_iter):\n",
    "    m_sample, n_feature = X.shape\n",
    "    theta = np.random.rand(n_feature).reshape(-1, n_feature)\n",
    "    iter_cost = []\n",
    "    for i in range(n_iter):\n",
    "        pred = logistic_prediction(X, theta)\n",
    "        change = []\n",
    "        for j in range(n_feature):\n",
    "            change.append((np.dot(X[:, j], (pred - y))) / m_sample)\n",
    "            theta[0][j] = theta[0][j] - alpha * change[j]\n",
    "        cost = abs(sum(change))\n",
    "        iter_cost.append(cost)\n",
    "    return theta[0], np.array(iter_cost)\n",
    "\n",
    "def calc_cross_entropy(X, y, theta):\n",
    "    m_sample = X.shape[0]\n",
    "    ones = np.ones(m_sample)\n",
    "    h = logistic_prediction(X, theta)\n",
    "    ce = -(np.dot(y.T, np.log(h)) + np.dot((ones - y).T, np.log(ones - h))) / m_sample\n",
    "    return ce[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6dda1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 VS 2:\n",
      "Train Cross Entropy: 0.57 | Test Cross Entropy: 0.58\n",
      "Train Accuracy: 71.68 | Test Accuracy: 71.04\n",
      "\n",
      "1 VS 3:\n",
      "Train Cross Entropy: 0.45 | Test Cross Entropy: 0.45\n",
      "Train Accuracy: 81.64 | Test Accuracy: 81.61\n",
      "\n",
      "2 VS 3:\n",
      "Train Cross Entropy: 0.57 | Test Cross Entropy: 0.57\n",
      "Train Accuracy: 70.69 | Test Accuracy: 71.17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Our Logistic Regression\n",
    "\n",
    "# Preparing Data\n",
    "X_1v2 = X_new[np.where(y != 3)]\n",
    "X_1v3 = X_new[np.where(y != 2)]\n",
    "X_2v3 = X_new[np.where(y != 1)]\n",
    "\n",
    "y_1v2 = y[y != 3]\n",
    "y_1v3 = y[y != 2]\n",
    "y_2v3 = y[y != 1]\n",
    "\n",
    "y_1v2 = np.array([0 if y == 1 else 1 for y in y_1v2]).reshape(-1, 1)\n",
    "y_1v3 = np.array([0 if y == 1 else 1 for y in y_1v3]).reshape(-1, 1)\n",
    "y_2v3 = np.array([0 if y == 2 else 1 for y in y_2v3]).reshape(-1, 1)\n",
    "\n",
    "X_train_1v2, X_test_1v2, y_train_1v2, y_test_1v2 = train_test_split(X_1v2, y_1v2, test_size=0.2, random_state=42)\n",
    "X_train_1v3, X_test_1v3, y_train_1v3, y_test_1v3 = train_test_split(X_1v3, y_1v3, test_size=0.2, random_state=42)\n",
    "X_train_2v3, X_test_2v3, y_train_2v3, y_test_2v3 = train_test_split(X_2v3, y_2v3, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Phase Of 1 VS 2\n",
    "theta_1v2, cost_1v2 = logistic_gradient(X_train_1v2, y_train_1v2, 0.1, 10000)\n",
    "\n",
    "# Test Phase Of 1 VS 2\n",
    "yh_train_1v2 = logistic_prediction(X_train_1v2, theta_1v2, norm=True)\n",
    "yh_test_1v2 = logistic_prediction(X_test_1v2, theta_1v2, norm=True)\n",
    "\n",
    "train_ce_1v2 = calc_cross_entropy(X_train_1v2, y_train_1v2, theta_1v2)\n",
    "test_ce_1v2 = calc_cross_entropy(X_test_1v2, y_test_1v2, theta_1v2)\n",
    "\n",
    "train_acc_1v2 = accuracy_score(y_train_1v2, yh_train_1v2)\n",
    "test_acc_1v2 = accuracy_score(y_test_1v2, yh_test_1v2)\n",
    "\n",
    "# Results For 1 VS 2\n",
    "print('1 VS 2:')\n",
    "print(f'Train Cross Entropy: {train_ce_1v2.round(2)} | Test Cross Entropy: {test_ce_1v2.round(2)}')\n",
    "print(f'Train Accuracy: {round(train_acc_1v2 * 100, 2)} | Test Accuracy: {round(test_acc_1v2 * 100, 2)}\\n')\n",
    "\n",
    "# Train Phase Of 1 VS 3\n",
    "theta_1v3, cost_1v3 = logistic_gradient(X_train_1v3, y_train_1v3, 0.1, 10000)\n",
    "\n",
    "# Test Phase Of 1 VS 3\n",
    "yh_train_1v3 = logistic_prediction(X_train_1v3, theta_1v3, norm=True)\n",
    "yh_test_1v3 = logistic_prediction(X_test_1v3, theta_1v3, norm=True)\n",
    "\n",
    "train_ce_1v3 = calc_cross_entropy(X_train_1v3, y_train_1v3, theta_1v3)\n",
    "test_ce_1v3 = calc_cross_entropy(X_test_1v3, y_test_1v3, theta_1v3)\n",
    "\n",
    "train_acc_1v3 = accuracy_score(y_train_1v3, yh_train_1v3)\n",
    "test_acc_1v3 = accuracy_score(y_test_1v3, yh_test_1v3)\n",
    "\n",
    "# Results For 1 VS 3\n",
    "print('1 VS 3:')\n",
    "print(f'Train Cross Entropy: {train_ce_1v3.round(2)} | Test Cross Entropy: {test_ce_1v3.round(2)}')\n",
    "print(f'Train Accuracy: {round(train_acc_1v3 * 100, 2)} | Test Accuracy: {round(test_acc_1v3 * 100, 2)}\\n')\n",
    "\n",
    "# Train Phase Of 2 VS 3\n",
    "theta_2v3, cost_2v3 = logistic_gradient(X_train_2v3, y_train_2v3, 0.1, 10000)\n",
    "\n",
    "# Test Phase Of 2 VS 3\n",
    "yh_train_2v3 = logistic_prediction(X_train_2v3, theta_2v3, norm=True)\n",
    "yh_test_2v3 = logistic_prediction(X_test_2v3, theta_2v3, norm=True)\n",
    "\n",
    "train_ce_2v3 = calc_cross_entropy(X_train_2v3, y_train_2v3, theta_2v3)\n",
    "test_ce_2v3 = calc_cross_entropy(X_test_2v3, y_test_2v3, theta_2v3)\n",
    "\n",
    "train_acc_2v3 = accuracy_score(y_train_2v3, yh_train_2v3)\n",
    "test_acc_2v3 = accuracy_score(y_test_2v3, yh_test_2v3)\n",
    "\n",
    "# Results For 2 VS 3\n",
    "print('2 VS 3:')\n",
    "print(f'Train Cross Entropy: {train_ce_2v3.round(2)} | Test Cross Entropy: {test_ce_2v3.round(2)}')\n",
    "print(f'Train Accuracy: {round(train_acc_2v3 * 100, 2)} | Test Accuracy: {round(test_acc_2v3 * 100, 2)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70a3d2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1V2 Train Confusion Matrix:\n",
      " [[2952 1542]\n",
      " [1008 3501]]\n",
      "\n",
      "1V3 Train Confusion Matrix:\n",
      " [[3430 1079]\n",
      " [ 574 3920]]\n",
      "\n",
      "2V3 Train Confusion Matrix:\n",
      " [[2747 1762]\n",
      " [ 877 3617]]\n",
      "\n",
      "1V2 Test Confusion Matrix:\n",
      " [[752 381]\n",
      " [271 847]]\n",
      "\n",
      "1V3 Test Confusion Matrix:\n",
      " [[ 830  288]\n",
      " [ 126 1007]]\n",
      "\n",
      "2V3 Test Confusion Matrix:\n",
      " [[683 435]\n",
      " [214 919]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Confusion Matrix\n",
    "print(f'1V2 Train Confusion Matrix:\\n {confusion_matrix(y_train_1v2, yh_train_1v2)}\\n')\n",
    "print(f'1V3 Train Confusion Matrix:\\n {confusion_matrix(y_train_1v3, yh_train_1v3)}\\n')\n",
    "print(f'2V3 Train Confusion Matrix:\\n {confusion_matrix(y_train_2v3, yh_train_2v3)}\\n')\n",
    "\n",
    "# Test Confusion Matrix\n",
    "print(f'1V2 Test Confusion Matrix:\\n {confusion_matrix(y_test_1v2, yh_test_1v2)}\\n')\n",
    "print(f'1V3 Test Confusion Matrix:\\n {confusion_matrix(y_test_1v3, yh_test_1v3)}\\n')\n",
    "print(f'2V3 Test Confusion Matrix:\\n {confusion_matrix(y_test_2v3, yh_test_2v3)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92a73ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Functions\n",
    "def one_hot_encode(y):\n",
    "    m_sample = len(y)\n",
    "    classes = np.unique(y)\n",
    "    c_class = len(classes)\n",
    "    one_hot = np.zeros((m_sample, c_class), int)\n",
    "    for i in range(m_sample):\n",
    "        for j, c in enumerate(classes):\n",
    "            if y[i] == c:\n",
    "                one_hot[i][j] = 1\n",
    "    return one_hot\n",
    "\n",
    "def softmax_prediction(X, theta, norm=False):\n",
    "    z = np.dot(X, theta)\n",
    "    h = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    yh = h / np.sum(h, axis=1, keepdims=True)\n",
    "    if not norm:\n",
    "        return yh\n",
    "    else:\n",
    "        return np.argmax(yh, axis=1, keepdims=True) + 1\n",
    "\n",
    "def softmax_gradient(X, y, alpha, n_iter):\n",
    "    m_sample, n_feature = X.shape\n",
    "    c_class = len(np.unique(y))\n",
    "    theta = np.random.rand(n_feature, c_class)\n",
    "    one_hot = one_hot_encode(np.array(y))\n",
    "    iter_cost = []\n",
    "    for i in range(n_iter):\n",
    "        pred = softmax_prediction(X, theta)\n",
    "        change = np.dot(X.T, (pred - one_hot))\n",
    "        theta = theta - alpha * np.array(change)\n",
    "        cost = 0\n",
    "        for c in range(c_class):\n",
    "            cost += np.sum(one_hot[:, c] * np.log10(pred[:, c]))\n",
    "        cost = -cost / m_sample\n",
    "        iter_cost.append(cost)\n",
    "    return theta, iter_cost[-1].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a78694aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Regression Result:\n",
      "Cross Entropy: 0.32\n",
      "Train Accuracy: 67.17 | Test Accuracy: 66.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Our Softmax\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
    "\n",
    "class_theta, cross_entropy = softmax_gradient(X_train, y_train, 0.01, 100)\n",
    "\n",
    "yh_train = softmax_prediction(X_train, class_theta, True)\n",
    "yh_test = softmax_prediction(X_test, class_theta, True)\n",
    "\n",
    "acc_train = accuracy_score(np.array(y_train).reshape(-1, 1), yh_train)\n",
    "acc_test = accuracy_score(np.array(y_test).reshape(-1, 1), yh_test)\n",
    "\n",
    "print('Softmax Regression Result:')\n",
    "print(f'Cross Entropy: {cross_entropy}')\n",
    "print(f'Train Accuracy: {round(acc_train * 100, 2)} | Test Accuracy: {round(acc_test * 100, 2)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eef65896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Confusion Matrix:\n",
      " [[3260  813  379]\n",
      " [1075 2514  959]\n",
      " [ 450  757 3297]]\n",
      "\n",
      "Test Confusion Matrix:\n",
      " [[858 222  95]\n",
      " [258 573 248]\n",
      " [102 196 825]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Confusion Matrix:\\n {confusion_matrix(y_train, yh_train)}\\n')\n",
    "print(f'Test Confusion Matrix:\\n {confusion_matrix(y_test, yh_test)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05a222c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy Using Library: 67.76\n",
      "Test Accuracy Using Library: 67.25\n",
      "\n",
      "Train Confusion Matrix:\n",
      "[[3090 1080  282]\n",
      " [ 849 2920  779]\n",
      " [ 362 1002 3140]]\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[815 293  67]\n",
      " [206 675 198]\n",
      " [ 99 243 781]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scikit-Learn Logistic Regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=250)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f'Train Accuracy Using Library: {round(train_accuracy * 100, 2)}')\n",
    "print(f'Test Accuracy Using Library: {round(test_accuracy * 100, 2)}')\n",
    "print()\n",
    "print(f'Train Confusion Matrix:\\n{confusion_matrix(y_train, y_train_pred)}\\n')\n",
    "print(f'Test Confusion Matrix:\\n{confusion_matrix(y_test, y_test_pred)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f3d4efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Functions\n",
    "def class_probability(label):\n",
    "    m_sample = len(label)\n",
    "    _, count = np.unique(label, return_counts=True)\n",
    "    class_prob = count / m_sample\n",
    "    return class_prob\n",
    "\n",
    "def index_probability(data, label):\n",
    "    unique, count = np.unique(label, return_counts=True)\n",
    "    c_class = len(unique)\n",
    "    m_sample = len(label)\n",
    "    n_feature = 28 * 28\n",
    "    index_prob = np.zeros((n_feature, c_class, 2))\n",
    "    for m in range(m_sample):\n",
    "        for i in range(300):\n",
    "            if data[m][i] == 0:\n",
    "                index_prob[i][label[m]][0] += 1\n",
    "            elif data[m][i] != 0:\n",
    "                index_prob[i][label[m]][1] += 1\n",
    "    for n in range(n_feature):\n",
    "        for c in range(c_class):\n",
    "            for v in range(2):\n",
    "                index_prob[n][c][v] = index_prob[n][c][v] / count[c]\n",
    "    return index_prob\n",
    "\n",
    "def naive_bayes_prediction(data, c_prob, i_prob):\n",
    "    c_class = len(c_prob)\n",
    "    m_sample = len(data)\n",
    "    data_pred = np.zeros((m_sample, 1), int)\n",
    "    for i, d in enumerate(data):\n",
    "        pred_arr = c_prob.copy()\n",
    "        for j in range(300):\n",
    "            for m in range(c_class):\n",
    "                if d[j] == 0:\n",
    "                    pred_arr[m] *= i_prob[j][m][0]\n",
    "                elif d[j] != 0:\n",
    "                    pred_arr[m] *= i_prob[j][m][1]\n",
    "        data_pred[i][0] = np.argmax(pred_arr)\n",
    "    return data_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3da6f7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 65.54\n",
      "Test Accuracy: 66.48\n"
     ]
    }
   ],
   "source": [
    "# Our Naive Bayes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
    "y_train = np.array(y_train.astype(int) - 1)\n",
    "y_test = np.array(y_test.astype(int) - 1)\n",
    "\n",
    "class_prob = class_probability(y_train)\n",
    "index_prob = index_probability(X_train.tolist(), y_train)\n",
    "\n",
    "data_pred_train = naive_bayes_prediction(X_train, class_prob, index_prob)\n",
    "data_pred_test = naive_bayes_prediction(X_test, class_prob, index_prob)\n",
    "\n",
    "naive_bayes_acc_train = accuracy_score(y_train.reshape(-1, 1), data_pred_train.reshape(-1, 1))\n",
    "naive_bayes_acc_test = accuracy_score(y_test.reshape(-1, 1), data_pred_test.reshape(-1, 1))\n",
    "\n",
    "print('Train Accuracy: ' + str(round(naive_bayes_acc_train * 100, 2)))\n",
    "print('Test Accuracy: ' + str(round(naive_bayes_acc_test * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8b38c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Confusion Matrix:\n",
      " [[3270  780  402]\n",
      " [1168 2328 1052]\n",
      " [ 505  746 3253]]\n",
      "\n",
      "Test Confusion Matrix:\n",
      " [[873 207  95]\n",
      " [279 546 254]\n",
      " [105 192 826]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Confusion Matrix:\\n {confusion_matrix(y_train, data_pred_train)}\\n')\n",
    "print(f'Test Confusion Matrix:\\n {confusion_matrix(y_test, data_pred_test)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "559b765f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 66.69\n",
      "Test Accuracy 66.12\n",
      "\n",
      "Train Confusion Matrix:\n",
      " [[2917 1115  420]\n",
      " [ 752 2884  912]\n",
      " [ 300  999 3205]]\n",
      "\n",
      "Test Confusion Matrix:\n",
      " [[772 304  99]\n",
      " [195 656 228]\n",
      " [ 79 239 805]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scikit-Learn Naive Bayes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "train_accuracy = clf.score(X_train, y_train)\n",
    "test_accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "print(f'Train Accuracy {round(train_accuracy * 100, 2)}')\n",
    "print(f'Test Accuracy {round(test_accuracy * 100, 2)}')\n",
    "print()\n",
    "print(f'Train Confusion Matrix:\\n {confusion_matrix(y_train, clf.predict(X_train))}\\n')\n",
    "print(f'Test Confusion Matrix:\\n {confusion_matrix(y_test, clf.predict(X_test))}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5eff03d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our SVM\n",
    "def svm (X,Y,C):\n",
    "    learning_rate = 0.01\n",
    "    _samples,_features = X.shape\n",
    "    w = np.zeros((1, _features))\n",
    "    b = 0\n",
    "    for i in range(600):\n",
    "        gradw = 0\n",
    "        gradb = 0\n",
    "        for idx, x_i in enumerate(X):\n",
    "            if Y[idx] * (np.dot(w, x_i.T) + b)> 1:\n",
    "                gradw += 0\n",
    "                gradb += 0\n",
    "            else:\n",
    "                gradw += C * Y[idx] * x_i\n",
    "                gradb += C * Y[idx]  \n",
    "        w = w - learning_rate * w + learning_rate * gradw\n",
    "        b = b + learning_rate * gradb\n",
    "    return w,b\n",
    "\n",
    "def predict(X,w,b):\n",
    "    prediction = np.dot(X, w[0]) + b \n",
    "    return np.sign(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef88c154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  39.41\n"
     ]
    }
   ],
   "source": [
    "# Our SVM\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Preparing Data\n",
    "X_1v2 = X_train[np.where(y_train != 3)]\n",
    "X_1v3 = X_train[np.where(y_train != 2)]\n",
    "X_2v3 = X_train[np.where(y_train != 1)]\n",
    "\n",
    "y_1v2 = y_train[y_train != 3]\n",
    "y_1v3 = y_train[y_train != 2]\n",
    "y_2v3 = y_train[y_train != 1]\n",
    "\n",
    "y_1v2 = np.array([-1 if y == 1 else 1 for y in y_1v2]).reshape(-1, 1)\n",
    "y_1v3 = np.array([-1 if y == 1 else 1 for y in y_1v3]).reshape(-1, 1)\n",
    "y_2v3 = np.array([-1 if y == 2 else 1 for y in y_2v3]).reshape(-1, 1)\n",
    "\n",
    "w1V2, b1v2 = svm(X_1v2, y_1v2, 100)\n",
    "w1V3, b1v3 = svm(X_1v3, y_1v3, 100)\n",
    "w2V3, b2v3 = svm(X_2v3, y_2v3, 100)\n",
    "\n",
    "y_pre1v2 = predict(X_test, w1V2, b1v2)\n",
    "y_pre1v3 = predict(X_test, w1V3, b1v3)\n",
    "y_pre2v3 = predict(X_test, w2V3, b2v3)\n",
    "\n",
    "y_pre1v2 = [1 if y == -1 else 2 for y in y_pre1v2]\n",
    "y_pre1v3 = [1 if y == -1 else 3 for y in y_pre1v3]\n",
    "y_pre2v3 = [2 if y == -1 else 3 for y in y_pre2v3]\n",
    "\n",
    "# Maximum Majority \n",
    "y_pre = []\n",
    "for y in range(len(y_pre1v2)):\n",
    "    temp = []\n",
    "    temp.append(y_pre1v2[y])\n",
    "    temp.append(y_pre1v3[y])\n",
    "    temp.append(y_pre2v3[y])\n",
    "    label_1 = temp.count(1)\n",
    "    label_2 = temp.count(2)\n",
    "    label_3 = temp.count(3)\n",
    "    if label_1 == max(label_1, label_2, label_3):\n",
    "        y_pre.append(1)\n",
    "        continue\n",
    "    if label_2 == max(label_1, label_2, label_3):\n",
    "        y_pre.append(2) \n",
    "        continue\n",
    "    if label_3 == max(label_1, label_2, label_3):\n",
    "        y_pre.append(3) \n",
    "        continue\n",
    "        \n",
    "accuracy = accuracy_score(y_test, y_pre) \n",
    "\n",
    "print(\"Accuracy: \", round(accuracy * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf6c8a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1173    2    0]\n",
      " [1064    4   11]\n",
      " [ 967    2  154]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c647a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  66.92\n",
      "[[781 337  57]\n",
      " [163 738 178]\n",
      " [ 84 298 741]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear', decision_function_shape='ovr')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", round(accuracy * 100, 2))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
